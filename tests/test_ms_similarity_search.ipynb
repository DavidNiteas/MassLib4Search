{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "95bba618",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import dask\n",
    "import dask.bag as db\n",
    "from dask.diagnostics import ProgressBar\n",
    "import pandas as pd\n",
    "import ms_entropy as me\n",
    "from typing import Tuple,Callable,Optional,Union,Literal,List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "ca235268",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ms_entropy_similarity(\n",
    "    query_spec: torch.Tensor, # (n_peaks, 2)\n",
    "    ref_spec: torch.Tensor, # (n_peaks, 2)\n",
    ") -> torch.Tensor: # zero-dimensional\n",
    "    print(query_spec.shape, ref_spec.shape)\n",
    "    sim = me.calculate_entropy_similarity(query_spec, ref_spec)\n",
    "    return torch.tensor(sim, device=query_spec.device)\n",
    "\n",
    "@torch.no_grad()\n",
    "def spec_similarity_search_cpu(\n",
    "    query: List[torch.Tensor], # List[(n_peaks, 2)]\n",
    "    ref: List[torch.Tensor], # List[(n_peaks, 2)]\n",
    "    sim_operator: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = ms_entropy_similarity,\n",
    "    top_k: Optional[int] = None,\n",
    "    num_dask_workers: int = 4,\n",
    "    work_device: torch.device = torch.device(\"cpu\"),\n",
    "    output_device: Optional[torch.device] = None,\n",
    "    dask_mode: Optional[Literal[\"threads\", \"processes\", \"single-threaded\"]] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    output_device = output_device or work_device\n",
    "    top_k = top_k or len(ref)\n",
    "    \n",
    "    # 缓冲区模板\n",
    "    scores_template = torch.full((top_k,), -float('inf'), \n",
    "                                device=work_device, dtype=torch.float32)\n",
    "    indices_template = torch.full((top_k,), -1, \n",
    "                                device=work_device, dtype=torch.long)\n",
    "    \n",
    "    # 单query搜索闭包\n",
    "    def _search_single_query(q: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        # 初始化缓冲区\n",
    "        scores_buf = scores_template.clone()\n",
    "        indices_buf = indices_template.clone()\n",
    "        current_count = 0  # 有效结果计数器\n",
    "        \n",
    "        q_tensor = q.to(work_device)\n",
    "        \n",
    "        for r_idx, r_spec in enumerate(ref):\n",
    "            score = sim_operator(q_tensor, r_spec.to(work_device))\n",
    "            \n",
    "            # 仅处理可能进入TopK的情况\n",
    "            if score > scores_buf.min() or current_count < top_k:\n",
    "                # 合并到临时缓冲区\n",
    "                temp_scores = torch.cat([scores_buf[:current_count], score.unsqueeze(0)])\n",
    "                temp_indices = torch.cat([indices_buf[:current_count], \n",
    "                                        torch.tensor([r_idx], device=work_device)])\n",
    "                \n",
    "                # 获取排序后的索引\n",
    "                sorted_idx = torch.argsort(temp_scores, descending=True)\n",
    "                \n",
    "                # 更新主缓冲区\n",
    "                keep = min(top_k, len(temp_scores))\n",
    "                scores_buf[:keep] = temp_scores[sorted_idx][:keep]\n",
    "                indices_buf[:keep] = temp_indices[sorted_idx][:keep]\n",
    "                current_count = min(current_count + 1, top_k)\n",
    "\n",
    "        return scores_buf.to(output_device), indices_buf.to(output_device)\n",
    "\n",
    "    # Dask并行处理\n",
    "    query_bag = db.from_sequence(query, npartitions=num_dask_workers)\n",
    "    query_bag = query_bag.map(_search_single_query)\n",
    "    results = query_bag.compute(scheduler=dask_mode,num_workers=num_dask_workers)\n",
    "    \n",
    "    # 堆叠结果\n",
    "    results = pd.DataFrame(results,columns=[\"scores\", \"indices\"])\n",
    "    scores = torch.stack(results['scores'].tolist())\n",
    "    indices = torch.stack(results['indices'].tolist())\n",
    "    \n",
    "    return scores, indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "310be45f",
   "metadata": {},
   "outputs": [],
   "source": [
    "queries = [\n",
    "        torch.tensor([[100.0, 1.0], [200.0, 0.8], [300.0, 0.5]], dtype=torch.float32),\n",
    "        torch.tensor([[150.0, 0.9], [250.0, 0.7], [350.0, 0.6]], dtype=torch.float32),\n",
    "        torch.tensor([[150.0, 0.9], [200.0, 0.7], [300.0, 0.6]], dtype=torch.float32),\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "773608b8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 238.75 us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 102.52 ms\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    S,I = spec_similarity_search_cpu(queries, queries, top_k=2, num_dask_workers=2, dask_mode='threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "2fe6560a",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[1.0000, 0.6201],\n",
       "        [1.0000, 0.3722],\n",
       "        [1.0000, 0.6201]])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "S"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "39dce39e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0, 2],\n",
       "        [1, 2],\n",
       "        [2, 0]])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "I"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "9a7b8f6e",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def spec_similarity_search_cpu_by_queue(\n",
    "    query: List[List[torch.Tensor]],  # Queue[List[(n_peaks, 2)]]\n",
    "    ref: List[List[torch.Tensor]], # Queue[List[(n_peaks, 2)]]\n",
    "    sim_operator: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = ms_entropy_similarity,\n",
    "    top_k: Optional[int] = None,\n",
    "    num_dask_workers: int = 4,\n",
    "    work_device: torch.device = torch.device(\"cpu\"),\n",
    "    output_device: Optional[torch.device] = None,\n",
    "    dask_mode: Optional[Literal[\"threads\", \"processes\", \"single-threaded\"]] = None,\n",
    ") -> List[Tuple[torch.Tensor, torch.Tensor]]:\n",
    "\n",
    "    output_device = output_device or work_device\n",
    "    top_k = top_k or len(ref)\n",
    "    \n",
    "    # 缓冲区模板\n",
    "    scores_template = torch.full((top_k,), -float('inf'), \n",
    "                                device=work_device, dtype=torch.float32)\n",
    "    indices_template = torch.full((top_k,), -1, \n",
    "                                device=work_device, dtype=torch.long)\n",
    "    \n",
    "    # 单query搜索闭包\n",
    "    def _search_single_query(i: int, q: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "        \n",
    "        # 初始化缓冲区\n",
    "        scores_buf = scores_template.clone()\n",
    "        indices_buf = indices_template.clone()\n",
    "        current_count = 0  # 有效结果计数器\n",
    "        \n",
    "        q_tensor = q.to(work_device)\n",
    "        \n",
    "        for r_idx, r_spec in enumerate(ref[i]):\n",
    "            score = sim_operator(q_tensor, r_spec.to(work_device))\n",
    "            \n",
    "            # 仅处理可能进入TopK的情况\n",
    "            if score > scores_buf.min() or current_count < top_k:\n",
    "                # 合并到临时缓冲区\n",
    "                temp_scores = torch.cat([scores_buf[:current_count], score.unsqueeze(0)])\n",
    "                temp_indices = torch.cat([indices_buf[:current_count], \n",
    "                                        torch.tensor([r_idx], device=work_device)])\n",
    "                \n",
    "                # 获取排序后的索引\n",
    "                sorted_idx = torch.argsort(temp_scores, descending=True)\n",
    "                \n",
    "                # 更新主缓冲区\n",
    "                keep = min(top_k, len(temp_scores))\n",
    "                scores_buf[:keep] = temp_scores[sorted_idx][:keep]\n",
    "                indices_buf[:keep] = temp_indices[sorted_idx][:keep]\n",
    "                current_count = min(current_count + 1, top_k)\n",
    "\n",
    "        return scores_buf.to(output_device), indices_buf.to(output_device)\n",
    "    \n",
    "    # 构建配对序列\n",
    "    bag_queue = []\n",
    "    for i,query_block in enumerate(query):\n",
    "        query_block_bag = db.from_sequence(zip([i]*len(query_block), query_block), npartitions=num_dask_workers)\n",
    "        results_bag = query_block_bag.map(lambda x: _search_single_query(x[0], x[1]))\n",
    "        bag_queue.append(results_bag)\n",
    "    \n",
    "    # 并行搜索\n",
    "    queue_results = dask.compute(bag_queue, scheduler=dask_mode, num_workers=num_dask_workers)[0]\n",
    "    \n",
    "    # 合并结果\n",
    "    queue_results_bag = db.from_sequence(queue_results, npartitions=num_dask_workers)\n",
    "    queue_results_bag = queue_results_bag.map(lambda x: pd.DataFrame(x,columns=[\"scores\", \"indices\"]))\n",
    "    queue_results_bag = queue_results_bag.map(lambda x: (torch.stack(x['scores'].tolist()),torch.stack(x['indices'].tolist())))\n",
    "    queue_results = queue_results_bag.compute(scheduler=dask_mode, num_workers=num_dask_workers)\n",
    "    \n",
    "    return queue_results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "id": "53093ca9",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[                                        ] | 0% Completed | 186.64 us"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[########################################] | 100% Completed | 101.99 ms\n",
      "[########################################] | 100% Completed | 101.83 ms\n"
     ]
    }
   ],
   "source": [
    "with ProgressBar():\n",
    "    queue_results = spec_similarity_search_cpu_by_queue([queries]*2, [queries]*2, top_k=2, num_dask_workers=2, dask_mode='threads')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "id": "10aec515",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[(tensor([[1.0000, 0.6201],\n",
       "          [1.0000, 0.3722],\n",
       "          [1.0000, 0.6201]]),\n",
       "  tensor([[0, 2],\n",
       "          [1, 2],\n",
       "          [2, 0]])),\n",
       " (tensor([[1.0000, 0.6201],\n",
       "          [1.0000, 0.3722],\n",
       "          [1.0000, 0.6201]]),\n",
       "  tensor([[0, 2],\n",
       "          [1, 2],\n",
       "          [2, 0]]))]"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "queue_results"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
