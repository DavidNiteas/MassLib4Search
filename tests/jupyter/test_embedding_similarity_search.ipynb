{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "61e4c5a4",
   "metadata": {},
   "outputs": [],
   "source": [
    "from masslib4search.snaps.MassSearchTools.utils.toolbox import EmbeddingSimilaritySearch\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac680947",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dask.bag as db\n",
    "from functools import partial\n",
    "from typing import Tuple,Callable,Optional,Union,Literal,List\n",
    "\n",
    "@torch.no_grad()\n",
    "def cosine(va: torch.Tensor, vb: torch.Tensor) -> torch.Tensor:\n",
    "    \"\"\"余弦相似度\"\"\"\n",
    "    norm_a = torch.norm(va, p=2, dim=-1, keepdim=True)\n",
    "    norm_b = torch.norm(vb, p=2, dim=-1, keepdim=True)\n",
    "    return torch.matmul(va, vb.transpose(-1,-2)) / (norm_a * norm_b.transpose(-1,-2) + 1e-6)\n",
    "\n",
    "@torch.no_grad()\n",
    "def emb_similarity_search_cpu(\n",
    "    query: torch.Tensor, # shape: (n_q, dim), dtype: float32\n",
    "    ref: torch.Tensor, # shape: (n_r, dim), dtype: float32\n",
    "    sim_operator: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = cosine,\n",
    "    top_k: Optional[int] = None,\n",
    "    chunk_size: int = 5120,\n",
    "    work_device: torch.device = torch.device(\"cpu\"),\n",
    "    output_device: Optional[torch.device] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # 设备配置\n",
    "    output_device = output_device or work_device\n",
    "    ref_num = ref.size(0)  # 获取参考集总数\n",
    "    top_k = ref_num if top_k is None else top_k  # 自动对齐参考集数量\n",
    "    \n",
    "    # 空查询集\n",
    "    if len(query) == 0:\n",
    "        return (\n",
    "            torch.tensor([], device=output_device, dtype=torch.long).reshape(0, top_k),\n",
    "            torch.tensor([], device=output_device, dtype=torch.float32).reshape(0, top_k)\n",
    "        )\n",
    "        \n",
    "    # 空参考集\n",
    "    if len(ref) == 0:\n",
    "        return (\n",
    "            torch.full((len(query),top_k), -1, device=output_device, dtype=torch.long),\n",
    "            torch.full((len(query),top_k), -float('inf'), device=output_device, dtype=torch.float32),\n",
    "        )\n",
    "    \n",
    "    # 初始化全局缓冲区模板\n",
    "    scores_template = torch.full((top_k,), -float('inf'), \n",
    "                                device=work_device, dtype=torch.float32)\n",
    "    indices_template = torch.full((top_k,), -1, \n",
    "                                device=work_device, dtype=torch.long)\n",
    "    \n",
    "    results = []\n",
    "    indices_list = []\n",
    "\n",
    "    # 分块处理查询集\n",
    "    for q_chunk in query.split(chunk_size):\n",
    "        q_work = q_chunk.to(work_device)\n",
    "        batch_size = q_work.size(0)\n",
    "        \n",
    "        # 初始化每批查询的缓冲区 (batch_size, top_k)\n",
    "        scores_buf = scores_template[None, :].expand(batch_size, -1).clone()\n",
    "        indices_buf = indices_template[None, :].expand(batch_size, -1).clone()\n",
    "\n",
    "        # 分块处理参考集\n",
    "        for r_idx, r_chunk in enumerate(ref.split(chunk_size)):\n",
    "            r_work = r_chunk.to(work_device)\n",
    "            sim = sim_operator(q_work, r_work)  # (batch_size, ref_chunk_size)\n",
    "            \n",
    "            # 生成全局索引\n",
    "            start_idx = r_idx * chunk_size\n",
    "            indices = torch.arange(start_idx, start_idx + r_work.size(0), \n",
    "                                    device=work_device)\n",
    "            \n",
    "            # 向量化合并逻辑\n",
    "            combined_scores = torch.cat([scores_buf, sim], dim=1)\n",
    "            combined_indices = torch.cat([\n",
    "                indices_buf, \n",
    "                indices.expand(batch_size, -1)\n",
    "            ], dim=1)\n",
    "            \n",
    "            # 保留TopK\n",
    "            top_scores, top_pos = torch.topk(combined_scores, top_k, dim=1)\n",
    "            scores_buf = top_scores\n",
    "            indices_buf = torch.gather(combined_indices, 1, top_pos)\n",
    "\n",
    "        # 后处理：确保严格排序（仅在需要时）\n",
    "        if top_k < ref_num:\n",
    "            sorted_idx = torch.argsort(scores_buf, dim=1, descending=True)\n",
    "            scores_buf = torch.gather(scores_buf, 1, sorted_idx)\n",
    "            indices_buf = torch.gather(indices_buf, 1, sorted_idx)\n",
    "        \n",
    "        # 转移结果到目标设备\n",
    "        results.append(scores_buf.to(output_device))\n",
    "        indices_list.append(indices_buf.to(output_device))\n",
    "\n",
    "    return torch.cat(indices_list, dim=0), torch.cat(results, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def emb_similarity_search_cuda(\n",
    "    query: torch.Tensor, # shape: (n_q, dim), dtype: float32\n",
    "    ref: torch.Tensor, # shape: (n_r, dim), dtype: float32\n",
    "    sim_operator: Callable[[torch.Tensor, torch.Tensor], torch.Tensor] = cosine,\n",
    "    top_k: Optional[int] = None,\n",
    "    chunk_size: int = 5120,\n",
    "    work_device: torch.device = torch.device(\"cuda:0\"),\n",
    "    output_device: Optional[torch.device] = None,\n",
    ") -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \n",
    "    # 设备配置\n",
    "    output_device = output_device or work_device\n",
    "    is_same_device = work_device == output_device\n",
    "    ref_num = ref.size(0)\n",
    "    top_k = ref_num if top_k is None else top_k  # 自动对齐参考集数量\n",
    "    \n",
    "    # 空查询集\n",
    "    if len(query) == 0:\n",
    "        return (\n",
    "            torch.tensor([], device=output_device, dtype=torch.long).reshape(0, top_k),\n",
    "            torch.tensor([], device=output_device, dtype=torch.float32).reshape(0, top_k)\n",
    "        )\n",
    "        \n",
    "    # 空参考集\n",
    "    if len(ref) == 0:\n",
    "        return (\n",
    "            torch.full((len(query),top_k), -1, device=output_device, dtype=torch.long),\n",
    "            torch.full((len(query),top_k), -float('inf'), device=output_device, dtype=torch.float32),\n",
    "        )\n",
    "    \n",
    "    # 初始化三流\n",
    "    input_stream = torch.cuda.Stream()\n",
    "    compute_stream = torch.cuda.Stream()\n",
    "    output_stream = torch.cuda.Stream()\n",
    "\n",
    "    # 数据预处理（异步）\n",
    "    with torch.cuda.stream(input_stream):\n",
    "        query = query.to(work_device, non_blocking=True)\n",
    "        ref = ref.to(work_device, non_blocking=True)\n",
    "    torch.cuda.synchronize()\n",
    "\n",
    "    # 缓冲区模板（固定内存）\n",
    "    with torch.cuda.stream(compute_stream):\n",
    "        scores_template = torch.full((top_k,), -float('inf'), \n",
    "                                    device=work_device, dtype=torch.float32)\n",
    "        indices_template = torch.full((top_k,), -1, \n",
    "                                    device=work_device, dtype=torch.long)\n",
    "\n",
    "    all_scores, all_indices = [], []\n",
    "    r_chunks = list(ref.split(chunk_size))\n",
    "\n",
    "    # 主处理循环\n",
    "    for q_chunk in query.split(chunk_size):\n",
    "        # 初始化缓冲区（每个查询块独立）\n",
    "        with torch.cuda.stream(compute_stream):\n",
    "            batch_size = q_chunk.size(0)\n",
    "            scores_buf = scores_template[None, :].expand(batch_size, -1).clone()\n",
    "            indices_buf = indices_template[None, :].expand(batch_size, -1).clone()\n",
    "\n",
    "        # 预加载第一个参考块\n",
    "        current_r = None\n",
    "        with torch.cuda.stream(input_stream):\n",
    "            current_r = r_chunks[0].to(work_device, non_blocking=True)\n",
    "        compute_events = [torch.cuda.Event() for _ in r_chunks]\n",
    "\n",
    "        for i in range(len(r_chunks)):\n",
    "            # 预加载下一个参考块\n",
    "            next_r = r_chunks[i+1].to(work_device, non_blocking=True) if i+1 < len(r_chunks) else None\n",
    "            with torch.cuda.stream(input_stream):\n",
    "                if next_r is not None:\n",
    "                    next_r = next_r.to(work_device, non_blocking=True)\n",
    "\n",
    "            # 计算块处理\n",
    "            with torch.cuda.stream(compute_stream):\n",
    "                compute_stream.wait_stream(input_stream)\n",
    "                \n",
    "                # 计算相似度\n",
    "                sim = sim_operator(q_chunk, current_r)\n",
    "                indices = torch.arange(\n",
    "                    i * chunk_size, \n",
    "                    i * chunk_size + current_r.size(0), \n",
    "                    device=work_device\n",
    "                )\n",
    "\n",
    "                # 合并到缓冲区\n",
    "                combined_scores = torch.cat([scores_buf, sim], dim=1)\n",
    "                combined_indices = torch.cat([\n",
    "                    indices_buf,\n",
    "                    indices[None, :].expand(batch_size, -1)\n",
    "                ], dim=1)\n",
    "                \n",
    "                # 保留TopK\n",
    "                scores_buf, top_pos = torch.topk(combined_scores, top_k, dim=1)\n",
    "                indices_buf = torch.gather(combined_indices, 1, top_pos)\n",
    "                \n",
    "                compute_events[i].record()\n",
    "\n",
    "            current_r = next_r\n",
    "\n",
    "        # 最终排序（非全量模式）\n",
    "        with torch.cuda.stream(compute_stream):\n",
    "            if top_k < ref_num:\n",
    "                sorted_idx = torch.argsort(scores_buf, dim=1, descending=True)\n",
    "                scores_buf = torch.gather(scores_buf, 1, sorted_idx)\n",
    "                indices_buf = torch.gather(indices_buf, 1, sorted_idx)\n",
    "\n",
    "        # 异步传输结果\n",
    "        with torch.cuda.stream(output_stream):\n",
    "            compute_events[-1].synchronize()\n",
    "            transfer = (scores_buf if is_same_device \n",
    "                        else scores_buf.to(output_device, non_blocking=True))\n",
    "            all_scores.append(transfer)\n",
    "            transfer = (indices_buf if is_same_device \n",
    "                        else indices_buf.to(output_device, non_blocking=True))\n",
    "            all_indices.append(transfer)\n",
    "\n",
    "    # 全局同步\n",
    "    torch.cuda.synchronize()\n",
    "    return (\n",
    "        torch.cat(all_indices, dim=0),\n",
    "        torch.cat(all_scores, dim=0)\n",
    "    )\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "id": "455fc7e1",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  1,  2,  0,  3, -1],\n",
       "         [ 0,  1,  3,  4,  2, -1]]),\n",
       " tensor([[0.7795, 0.7756, 0.7675, 0.7581, 0.7553,   -inf],\n",
       "         [0.7952, 0.7727, 0.7592, 0.7451, 0.7396,   -inf]]))"
      ]
     },
     "execution_count": 51,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_similarity_search_cpu(torch.rand(2, 128), torch.rand(5, 128),top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "id": "b413e2f7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[ 4,  1,  0,  3,  2, -1],\n",
       "         [ 3,  1,  0,  2,  4, -1]], device='cuda:0'),\n",
       " tensor([[0.7663, 0.7601, 0.7549, 0.7436, 0.7232,   -inf],\n",
       "         [0.8000, 0.7898, 0.7637, 0.7529, 0.7491,   -inf]], device='cuda:0'))"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_similarity_search_cuda(torch.rand(2, 128), torch.rand(5, 128),top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "id": "6fc2216d",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([], size=(0, 6), dtype=torch.int64), tensor([], size=(0, 6)))"
      ]
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_similarity_search_cpu(torch.tensor([]), torch.rand(5, 128),top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "id": "fe5cb87e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[-1, -1, -1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1, -1, -1],\n",
       "         [-1, -1, -1, -1, -1, -1]]),\n",
       " tensor([[-inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, -inf],\n",
       "         [-inf, -inf, -inf, -inf, -inf, -inf]]))"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "emb_similarity_search_cpu(torch.rand(3, 128), torch.tensor([]),top_k=6)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "841988d1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def convert_hits(indices: torch.Tensor, scores: torch.Tensor) -> Tuple[torch.Tensor, torch.Tensor]:\n",
    "    \"\"\"\n",
    "    将TopK格式的搜索结果转换为展开的命中列表格式\n",
    "    Args:\n",
    "        indices: shape (num_qry, top_k), dtype long，包含参考索引（-1表示无效）\n",
    "        scores:  shape (num_qry, top_k), dtype float，对应分数（-inf表示无效）\n",
    "    Returns:\n",
    "        new_indices: shape (num_hitted, 3)，每行格式 [qry_index, ref_index, top_k_pos]\n",
    "        new_scores:  shape (num_hitted,)，有效命中的分数\n",
    "    \"\"\"\n",
    "    # 生成查询索引网格\n",
    "    num_qry, top_k = indices.shape\n",
    "    qry_idx = torch.arange(num_qry, device=indices.device)[:, None].expand(-1, top_k)\n",
    "    topk_pos = torch.arange(top_k, device=indices.device)[None, :].expand(num_qry, -1)\n",
    "\n",
    "    # 创建有效命中掩码（排除ref_index=-1）\n",
    "    valid_mask = (indices != -1) & (scores != -float('inf'))\n",
    "\n",
    "    # 提取有效数据\n",
    "    valid_qry = qry_idx[valid_mask]\n",
    "    valid_ref = indices[valid_mask]\n",
    "    valid_topk = topk_pos[valid_mask]\n",
    "    valid_scores = scores[valid_mask]\n",
    "\n",
    "    # 组合结果\n",
    "    new_indices = torch.stack([valid_qry, valid_ref, valid_topk], dim=1)\n",
    "    return new_indices, valid_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "id": "9b52d608",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor([[0, 0, 0],\n",
       "         [0, 4, 1],\n",
       "         [0, 2, 2],\n",
       "         [0, 3, 3],\n",
       "         [0, 1, 4],\n",
       "         [1, 4, 0],\n",
       "         [1, 3, 1],\n",
       "         [1, 0, 2],\n",
       "         [1, 2, 3],\n",
       "         [1, 1, 4]]),\n",
       " tensor([0.7784, 0.7614, 0.7449, 0.7421, 0.7079, 0.7711, 0.7639, 0.7565, 0.7339,\n",
       "         0.6741]))"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "convert_hits(*emb_similarity_search_cpu(torch.rand(2, 128), torch.rand(5, 128),top_k=6))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
