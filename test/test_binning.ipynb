{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a504832f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import dask.bag as db\n",
    "from torch.nested import nested_tensor\n",
    "from typing import Literal,List,Optional,Tuple"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0d2897b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "@torch.no_grad()\n",
    "def infer_bin_cells(\n",
    "    min_mz: float = 50.0,\n",
    "    max_mz: float = 1000.0,\n",
    "    bin_size: float = 1,\n",
    ") -> torch.Tensor: # [num_bins, 2 (start, end)]\n",
    "    \n",
    "    starts = torch.arange(start=min_mz, end=max_mz, step=bin_size)\n",
    "    \n",
    "    ends = starts + bin_size\n",
    "    \n",
    "    return torch.stack((starts, ends), dim=1)\n",
    "\n",
    "@torch.no_grad()\n",
    "def binning_step(\n",
    "    padded_mzs: torch.Tensor,\n",
    "    padded_intensity: torch.Tensor,\n",
    "    bin_cells: torch.Tensor,\n",
    "    pool_method: Literal['sum','max', 'avg'] = \"sum\",\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    # 生成掩码张量 [n_spec, num_peaks, num_bins]\n",
    "    mask = (padded_mzs.unsqueeze(-1) >= bin_cells[:, 0]) & (padded_mzs.unsqueeze(-1) < bin_cells[:, 1])\n",
    "    mask = mask.float()\n",
    "    \n",
    "    # 批量池化计算\n",
    "    if pool_method == \"sum\":\n",
    "        return torch.einsum('spb,sp->sb', mask, padded_intensity)\n",
    "    elif pool_method == \"max\":\n",
    "        expanded_intensity = padded_intensity.unsqueeze(-1) * mask  # [n_spec, num_peaks, num_bins]\n",
    "        return torch.where(mask.any(dim=1), \n",
    "                          expanded_intensity.max(dim=1).values, \n",
    "                          torch.zeros_like(expanded_intensity[:,0,:]))\n",
    "    elif pool_method == \"avg\":\n",
    "        sum_result = torch.einsum('spb,sp->sb', mask, padded_intensity)\n",
    "        counts = mask.sum(dim=1)  # [n_spec, num_bins]\n",
    "        return sum_result / counts.clamp(min=1e-8)\n",
    "    else:\n",
    "        raise ValueError(f\"Unsupported pooling method: {pool_method}\")\n",
    "    \n",
    "@torch.no_grad()\n",
    "def binning_cpu(\n",
    "    mzs: List[torch.Tensor],\n",
    "    intensities: List[torch.Tensor],\n",
    "    bin_cells: torch.Tensor,\n",
    "    pool_method: Literal['sum','max', 'avg'] = \"sum\",\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 4,\n",
    ") -> torch.Tensor:\n",
    "    \n",
    "    # 参数校验\n",
    "    assert len(mzs) == len(intensities), \"M/Z与强度列表长度不一致\"\n",
    "\n",
    "    def process_batch(batch):\n",
    "        \n",
    "        mz_batch, intensity_batch = zip(*batch)\n",
    "        mz_batch = list(mz_batch)  # 转换为列表\n",
    "        intensity_batch = list(intensity_batch)  # 转换为列表\n",
    "        \n",
    "        nt_mz = nested_tensor(mz_batch)\n",
    "        nt_intensity = nested_tensor(intensity_batch)\n",
    "        \n",
    "        padded_mz = nt_mz.to_padded_tensor(0.0)\n",
    "        padded_intensity = nt_intensity.to_padded_tensor(0.0)\n",
    "        \n",
    "        return binning_step(padded_mz, padded_intensity, bin_cells, pool_method)\n",
    "\n",
    "    batches = [\n",
    "        list(zip(mzs[i:i+batch_size], intensities[i:i+batch_size]))\n",
    "        for i in range(0, len(mzs), batch_size)\n",
    "    ]\n",
    "\n",
    "    bag = db.from_sequence(batches, npartitions=num_workers)\n",
    "    results = bag.map(process_batch).compute()\n",
    "\n",
    "    return torch.cat(results, dim=0)\n",
    "\n",
    "@torch.no_grad()\n",
    "def binning_gpu(\n",
    "    mzs: List[torch.Tensor],\n",
    "    intensities: List[torch.Tensor],\n",
    "    bin_cells: torch.Tensor,\n",
    "    pool_method: Literal['sum','max', 'avg'] = \"sum\",\n",
    "    batch_size: int = 128,\n",
    "    num_workers: int = 4,\n",
    ") -> torch.Tensor:\n",
    "\n",
    "    # 参数校验\n",
    "    assert len(mzs) == len(intensities), \"M/Z与强度列表长度不一致\"\n",
    "    device = bin_cells.device\n",
    "    batch_size = batch_size or len(mzs)\n",
    "    num_workers = num_workers or 4\n",
    "\n",
    "    class Worker:\n",
    "        def __init__(self, device):\n",
    "            self.copy_stream = torch.cuda.Stream(device=device)  # 数据拷贝流\n",
    "            self.compute_stream = torch.cuda.Stream(device=device)  # 计算流\n",
    "            self.event = torch.cuda.Event()  # 用于流间同步\n",
    "\n",
    "    # 初始化worker池\n",
    "    workers = [Worker(device) for _ in range(num_workers)]\n",
    "    \n",
    "    # 预分配显存 (使用固定内存加速传输)\n",
    "    bin_cells_gpu = bin_cells.pin_memory().to(device, non_blocking=True)\n",
    "    pending_batches = []\n",
    "\n",
    "    def process_worker(batch, worker: Worker):\n",
    "        # 阶段1：在拷贝流执行数据传输\n",
    "        with torch.cuda.stream(worker.copy_stream):\n",
    "            # 异步拷贝数据到GPU\n",
    "            mz_batch = [t.to(device, non_blocking=True) for t, _ in batch]\n",
    "            intensity_batch = [t.to(device, non_blocking=True) for _, t in batch]\n",
    "            \n",
    "            # 执行padding\n",
    "            max_len = max(t.shape[0] for t in mz_batch)\n",
    "            padded_mz = torch.zeros(len(batch), max_len, device=device)\n",
    "            padded_intensity = torch.zeros_like(padded_mz)\n",
    "\n",
    "            for i, (mz, intensity) in enumerate(zip(mz_batch, intensity_batch)):\n",
    "                padded_mz[i, :len(mz)] = mz\n",
    "                padded_intensity[i, :len(intensity)] = intensity\n",
    "            \n",
    "            # 记录事件同步点\n",
    "            worker.event.record(worker.copy_stream)\n",
    "\n",
    "        # 阶段2：在计算流执行计算\n",
    "        with torch.cuda.stream(worker.compute_stream):\n",
    "            # 等待拷贝流完成\n",
    "            worker.event.wait(worker.compute_stream)\n",
    "\n",
    "            return binning_step(padded_mz, padded_intensity, bin_cells_gpu, pool_method)\n",
    "\n",
    "    # 流水线执行\n",
    "    for i in range(0, len(mzs), batch_size):\n",
    "        current_worker = workers[i % num_workers]\n",
    "        batch = list(zip(mzs[i:i+batch_size], intensities[i:i+batch_size]))\n",
    "        \n",
    "        # 提交任务到worker\n",
    "        future = process_worker(batch, current_worker)\n",
    "        pending_batches.append(future)\n",
    "\n",
    "    # 同步所有worker\n",
    "    for worker in workers:\n",
    "        torch.cuda.synchronize(worker.copy_stream)\n",
    "        torch.cuda.synchronize(worker.compute_stream)\n",
    "\n",
    "    return torch.cat(pending_batches, dim=0)\n",
    "\n",
    "def binning(\n",
    "    mzs: List[torch.Tensor],\n",
    "    intensities: List[torch.Tensor],\n",
    "    binning_window: Tuple[float,float,float] = (50.0, 1000.0, 1.0),\n",
    "    pool_method: Literal['sum','max', 'avg'] = \"sum\",\n",
    "    batch_size: Optional[int] = None,\n",
    "    num_workers: Optional[int] = None,\n",
    "    device: Literal['cpu', 'cuda'] = \"cpu\",\n",
    ") -> torch.Tensor:\n",
    "    \"\"\"\n",
    "    统一分箱处理入口函数\n",
    "    \n",
    "    参数：\n",
    "    - mzs: 质荷比列表，每个元素为形状[num_peaks]的张量\n",
    "    - intensities: 强度值列表，每个元素为形状[num_peaks]的张量\n",
    "    - binning_window: (min_mz, max_mz, bin_size) 分箱参数\n",
    "    - pool_method: 池化方法（sum/max/avg）\n",
    "    - batch_size: 批处理大小\n",
    "    - num_workers: 并行工作进程数\n",
    "    - device: 运行设备（cpu/cuda）\n",
    "    \n",
    "    返回：\n",
    "    - 分箱结果张量，形状为[num_spectra, num_bins]\n",
    "    \"\"\"\n",
    "    \n",
    "    # 参数校验\n",
    "    assert len(binning_window) == 3, \"分箱窗口需要包含三个参数（min_mz, max_mz, bin_size）\"\n",
    "    min_mz, max_mz, bin_size = binning_window\n",
    "    assert min_mz < max_mz, \"最小m/z必须小于最大m/z\"\n",
    "    assert bin_size > 0, \"分箱尺寸必须大于0\"\n",
    "    \n",
    "    # 生成分箱单元格\n",
    "    bin_cells = infer_bin_cells(min_mz, max_mz, bin_size)\n",
    "    \n",
    "    # 设备相关预处理\n",
    "    if device != 'cpu':\n",
    "        bin_cells = bin_cells.to(device)\n",
    "    \n",
    "    # 选择执行路径\n",
    "    if device == 'cpu':\n",
    "        return binning_cpu(\n",
    "            mzs=mzs,\n",
    "            intensities=intensities,\n",
    "            bin_cells=bin_cells,\n",
    "            pool_method=pool_method,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )\n",
    "    else:\n",
    "        return binning_gpu(\n",
    "            mzs=mzs,\n",
    "            intensities=intensities,\n",
    "            bin_cells=bin_cells,\n",
    "            pool_method=pool_method,\n",
    "            batch_size=batch_size,\n",
    "            num_workers=num_workers\n",
    "        )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "51d5e91a",
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "No CUDA GPUs are available",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 初始化数据\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mzs \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m)]\n\u001b[1;32m      3\u001b[0m intensities \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m)]\n\u001b[1;32m      4\u001b[0m bin_edges \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39munfold(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 2\u001b[0m, in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# 初始化数据\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m mzs \u001b[38;5;241m=\u001b[39m [\u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrand\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m1000\u001b[39;49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcuda\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m)]\n\u001b[1;32m      3\u001b[0m intensities \u001b[38;5;241m=\u001b[39m [torch\u001b[38;5;241m.\u001b[39mrand(\u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda() \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;241m10000\u001b[39m)]\n\u001b[1;32m      4\u001b[0m bin_edges \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mlinspace(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m1\u001b[39m, \u001b[38;5;241m1000\u001b[39m)\u001b[38;5;241m.\u001b[39mcuda()\u001b[38;5;241m.\u001b[39munfold(\u001b[38;5;241m0\u001b[39m, \u001b[38;5;241m2\u001b[39m, \u001b[38;5;241m1\u001b[39m)\n",
      "File \u001b[0;32m~/miniconda3/envs/MS310/lib/python3.10/site-packages/torch/cuda/__init__.py:319\u001b[0m, in \u001b[0;36m_lazy_init\u001b[0;34m()\u001b[0m\n\u001b[1;32m    317\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;129;01min\u001b[39;00m os\u001b[38;5;241m.\u001b[39menviron:\n\u001b[1;32m    318\u001b[0m     os\u001b[38;5;241m.\u001b[39menviron[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCUDA_MODULE_LOADING\u001b[39m\u001b[38;5;124m\"\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mLAZY\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 319\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_C\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_cuda_init\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    320\u001b[0m \u001b[38;5;66;03m# Some of the queued calls may reentrantly call _lazy_init();\u001b[39;00m\n\u001b[1;32m    321\u001b[0m \u001b[38;5;66;03m# we need to just return without initializing in that case.\u001b[39;00m\n\u001b[1;32m    322\u001b[0m \u001b[38;5;66;03m# However, we must not let any *other* threads in!\u001b[39;00m\n\u001b[1;32m    323\u001b[0m _tls\u001b[38;5;241m.\u001b[39mis_initializing \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: No CUDA GPUs are available"
     ]
    }
   ],
   "source": [
    "# 初始化数据\n",
    "mzs = [torch.rand(1000).cuda() for _ in range(10000)]\n",
    "intensities = [torch.rand(1000).cuda() for _ in range(10000)]\n",
    "bin_edges = torch.linspace(0, 1, 1000).cuda().unfold(0, 2, 1)\n",
    "\n",
    "# 执行优化版本\n",
    "result = binning_gpu(\n",
    "    mzs=mzs,\n",
    "    intensities=intensities,\n",
    "    bin_cells=bin_edges,\n",
    "    pool_method='max',\n",
    "    batch_size=512,\n",
    "    num_workers=8  # 根据GPU数量设置\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "MS310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
